{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road signs classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download and check data¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using GTSRB dataset (German Traffic Sign Recognition Benchmark) from http://benchmark.ini.rub.de/?section=gtsrb&subsection=news.\n",
    "Available datasets are train and test. We will split train set to train and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the first time\n",
    "#!wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip -P source_data\n",
    "#!wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip -P source_data\n",
    "#!wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip -P source_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the data to source_data. This will overwrite existing files and directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the first time\n",
    "\n",
    "#!unzip -o source_data/GTSRB_Final_Test_GT.zip -d source_data/test\n",
    "#!unzip -o source_data/GTSRB_Final_Test_Images.zip -d source_data/test\n",
    "#!unzip -o source_data/GTSRB_Final_Training_Images.zip -d source_data/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the data we will create PyTorch dataset and DataLoader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "currentDT = datetime.datetime.now()\n",
    "print (str(currentDT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTSRB train data format is directly supported by pytorch generic data loader ImageFolder: https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "\n",
    "#Create Transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "#    transforms.Resize((32, 32)),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('./source_data/train/GTSRB/Final_Training/Images', transform=transform_train)\n",
    "\n",
    "# Load Datasets\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect classes\n",
    "print(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test train_dataset\n",
    "assert len(train_dataset.classes) == 43, 'Incorrect number of classes, expected 43'\n",
    "assert len(train_dataset) == 39209, 'Incorrect number of train files, expected 39209'\n",
    "print('Train data loader correctly!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTSRB test data format is different: all image files are in one directory and there is a separate CSV file with ground truth labels, and some additinal features that we are not going to use.<br>\n",
    "\n",
    "```csv\n",
    "Filename;Width;Height;Roi.X1;Roi.Y1;Roi.X2;Roi.Y2;ClassId\n",
    "00000.ppm;53;54;6;5;48;49;16\n",
    "00001.ppm;42;45;5;5;36;40;1\n",
    "00002.ppm;48;52;6;6;43;47;38\n",
    "00003.ppm;27;29;5;5;22;24;33\n",
    "00004.ppm;60;57;5;5;55;52;11\n",
    "```\n",
    "<br>\n",
    "To load this data we will define a custom Dataset implementation iterating over the data, as defined in PyTorch Dataset documentation: https://pytorch.org/docs/stable/torchvision/datasets.html <br>\n",
    "\n",
    "There is one more custom change required: test classes use 2 digit data format and train data use 5 digits: 000nn.<br>\n",
    "We will use train data format: 000nn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize GTSRB_Test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GTSRB_Test import GTSRB_Test \n",
    "\n",
    "# Create Transform \n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create Dataset\n",
    "test_dataset = GTSRB_Test(\n",
    "    images_dir  = './source_data/test/GTSRB/Final_Test/Images', \n",
    "    gt_csv_path = './source_data/test/GT-final_test.csv',\n",
    "    transform = transform_train)\n",
    "\n",
    "# Load Dataset\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect classes\n",
    "print(test_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test test_dataset\n",
    "assert len(test_dataset) == 12630, 'Incorrect number of Validation files, expected 12630'\n",
    "assert len(test_dataset.class_to_idx) == 43, 'Incorrect number of classes, expected 43'\n",
    "assert len(set(train_dataset.classes) & set(test_dataset.classes)) == 43, 'Test and train classes do not match' \n",
    "\n",
    "print('Test data loaded correctly!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize sample train data¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros(len(train_dataset.classes))\n",
    "progress_bar = tqdm(range(len(train_dataset)), desc='Counting files in each class')\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    x,y = train_dataset[i]\n",
    "    labels[y] +=1\n",
    "    progress_bar.update()\n",
    "labels = np.array(labels, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for label in np.nditer(labels):\n",
    "    print('{}: {}'.format(i, label))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(32, 32))\n",
    "offset = 0\n",
    "for classId in range(len(train_dataset.classes)):\n",
    "    i = random.randint(0,labels[classId])\n",
    "    #offset to the right class\n",
    "    i = offset + i\n",
    "    plt.subplot(8, 8, classId+1)\n",
    "    plt.axis('off')\n",
    "    plt.title('Class: {}'.format(classId))\n",
    "    img, y = train_dataset[i]\n",
    "    inp = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    #plt.imshow(np.transpose(img.numpy(), (1,2,0)), interpolation='nearest')\n",
    "    offset += labels[classId]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation - train and validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains readme file with the following clarification:\n",
    "    \n",
    "    **********************************************\n",
    "    Image format and naming\n",
    "    **********************************************\n",
    "    The images are PPM images (RGB color). Files are numbered in two parts:\n",
    "\n",
    "       XXXXX_YYYYY.ppm\n",
    "\n",
    "    The first part, XXXXX, represents the track number. All images of one class \n",
    "    with identical track numbers originate from one single physical traffic sign.\n",
    "    The second part, YYYYY, is a running number within the track. The temporal order\n",
    "    of the images is preserved.\n",
    "    \n",
    "Train images come from short video clips so there are multiple images of the same sign, with slight variation to angle, size etc. If images of the same sign are used to both train and validate we risk data leakage and overfitting the model. For example, if particular road sign was damaged or had a sticker, we would want the model to learn to ignore such feature. If we use images with the same defect for both training and validation we would effectively train the model to recognise this defect as integral part of the class. Such model would score low in test.\n",
    "\n",
    "Below is an example of one such case of the sign with a sticker.\n",
    "\n",
    "This means that train and evaluation split needs to separate images by sign (\"track\") name prefix. In each track size there are always 30 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "for i in range(30):\n",
    "    plt.subplot(6,6,i+1)\n",
    "    index = str(i).zfill(5)\n",
    "    plt.axis('off')\n",
    "    img=mpimg.imread('./source_data/train/GTSRB/Final_Training/Images/00033/00001_'+index+'.ppm')\n",
    "    plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has high imbalance so we risk that the model would favour bigger classes as more probable based on input distribution, not features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Smallest class: '+str(min(labels)))\n",
    "print('Biggest class:  '+str(max(labels)))\n",
    "\n",
    "classes = np.arange(len(labels))\n",
    "plt.bar(classes, labels)\n",
    "plt.xlabel('Signs')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of signs in each class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to address it. One would be to add weights to loss function during training (for example https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss). However with over 10x difference between biggest and smallest classes, rare samples from small classes would be giving high error input to back propagation and the model might be difficult to train. <br>\n",
    "Another approach could be data augmentation based on the loss. Traffic signs are good candidates for augmentation because we can apply some limited affine transformations or rotate by small angles. <br>\n",
    "Ideally, the augmentation should be applied on training set to equalize class sizes. Before we do that let's take at one more aspect of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of differnt sign \"tracks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before signs images come from video tracks. Let's visualize the amount of tracks for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images_root = './source_data/train/GTSRB/Final_Training/Images'\n",
    "\n",
    "def get_track_count(input_images_root):\n",
    "    track_count = pd.DataFrame(columns=['class_id','track_count'])\n",
    "    with os.scandir(input_images_root) as images_it:\n",
    "        progress_bar = tqdm(range(43), desc='Counting tracks in each class')\n",
    "        for class_id in images_it:\n",
    "            if class_id.is_dir():# and class_id.name.startswith('00000'):\n",
    "                with os.scandir(input_images_root+'/'+class_id.name) as class_it:\n",
    "                    #print(input_images_root+'/'+class_id.name)\n",
    "                    df = pd.DataFrame(columns=['name'])\n",
    "                    for entry in class_it:\n",
    "                        if entry.is_file() and entry.name.endswith('.ppm'):\n",
    "                            df = df.append({'name':entry.name, 'track':entry.name[:entry.name.rfind(\"_\")]}, ignore_index=True) \n",
    "                    track_count = track_count.append({'class_id':class_id.name, 'track_count':len(df.track.unique())}, ignore_index=True)        \n",
    "            progress_bar.update()\n",
    "    track_count = track_count.sort_values(by=['class_id'])\n",
    "    return track_count\n",
    "\n",
    "track_count = get_track_count(input_images_root)\n",
    "track_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lowest number of tracks per class: '+str(min(track_count['track_count'])))\n",
    "print('Highest number of tracks per class:  '+str(max(track_count['track_count'])))\n",
    "\n",
    "classes = np.arange(len(track_count['track_count']))\n",
    "plt.bar(classes, track_count['track_count'], color='green')\n",
    "plt.xlabel('Tracks')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of tracks in each class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation by reuse and rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the overall size of the data set, when taking into account the correlation within tracks the data is smaller than it seems. As noticed by some researchers (for example [here](https://medium.com/@wolfapple/traffic-sign-recognition-2b0c3835e104), although the author made an error for the roundabout sign) it can be extended by rotating some signs or flipping and reusing for other class. For example left and right turn signs can be reused, and give way sign has horizontal symetry.<br>\n",
    "<br>\n",
    "Classes which can reuse images flipped horizontally from other class: (19,20), (33,34), (36,37), (38,39)<br>\n",
    "Classes with horizontal symmetry: 11, 12, 13, 15, 17, 18, 22, 26, 30, 35<br>\n",
    "Classes with vertical symmetry: 01, 05, 12, 15, 17<br>\n",
    "Classes which can be rotated by 120 or 240 degrees: 40<br>\n",
    "Classes which can be rotated by any degree: 15<br>\n",
    "Classes which diagonal symetry : 32<br>\n",
    "<br>\n",
    "Let's create a new data source directory which will include reused images.<br> \n",
    "I'll also change classes to 2 digits in the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first copy all .ppm files to new location using new class names\n",
    "input_images_root = './source_data/train/GTSRB/Final_Training/Images'\n",
    "output_images_root = './source_data_2/train'\n",
    "\n",
    "os.makedirs(output_images_root, exist_ok=True)\n",
    "with os.scandir(input_images_root) as images_it:\n",
    "    progress_bar = tqdm(range(43), desc='Copying files in each class')\n",
    "    for class_id in images_it:\n",
    "        if class_id.is_dir(): #and class_id.name.startswith('00000'):\n",
    "            os.makedirs(output_images_root+'/'+class_id.name[-2:], exist_ok=True)\n",
    "            with os.scandir(input_images_root+'/'+class_id.name) as class_it:\n",
    "                #print(input_images_root+'/'+class_id.name)\n",
    "                for entry in class_it:\n",
    "                    if entry.is_file() and entry.name.endswith('.ppm'):\n",
    "                        os.system('cp {} {}'.format(input_images_root+'/'+class_id.name+'/'+entry.name,\n",
    "                                 output_images_root+'/'+class_id.name[-2:]+'/'+entry.name))  \n",
    "            progress_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transforms\n",
    "\n",
    "def transpose_image(image_path, save_path, transpose_1st=None, transpose_2nd=None, rotate_1st=None, rotate_2nd=None):\n",
    "    \"\"\"\n",
    "    Transpose and rotate the given photo once or twice and save it\n",
    "    @param image_path: The path to the input image\n",
    "    @param transpose_1st: one of \n",
    "    @param transpose_2nd: one of \n",
    "        PIL.Image.FLIP_LEFT_RIGHT, \n",
    "        PIL.Image.FLIP_TOP_BOTTOM, \n",
    "        PIL.Image.ROTATE_90, \n",
    "        PIL.Image.ROTATE_180, \n",
    "        PIL.Image.ROTATE_270 or \n",
    "        PIL.Image.TRANSPOSE\n",
    "    @param rotate_1st: degree to rotate, anti-clockwise \n",
    "    @param rotate_2nd: degree to rotate, anti-clockwise \n",
    "    @param save_path: Path to save the image\n",
    "    \"\"\"\n",
    "    image_in = Image.open(image_path)\n",
    "    if transpose_1st != None:\n",
    "        image_out = image_in.transpose(transpose_1st)\n",
    "        image_in = image_out \n",
    "    if transpose_2nd != None:\n",
    "        image_out = image_in.transpose(transpose_2nd)\n",
    "        image_in = image_out \n",
    "    if rotate_1st != None:\n",
    "        image_out = image_in.rotate(rotate_1st)\n",
    "        image_in = image_out \n",
    "    if rotate_2nd != None:\n",
    "        image_out = image_in.rotate(rotate_2nd)\n",
    "    image_out.save(save_path)\n",
    "\n",
    "\n",
    "\n",
    "def transpose_class(class_id, prefix, transpose_1st=None, transpose_2nd=None, rotate_1st=None, rotate_2nd=None, together=True):\n",
    "    with os.scandir(output_images_root+'/'+class_id) as class_it:\n",
    "        if together:\n",
    "            #apply both transforms or rotations to each file\n",
    "            for entry in class_it:\n",
    "                image_path = output_images_root+'/'+class_id+'/'+entry.name\n",
    "                save_path = output_images_root+'/'+class_id+'/'+prefix+'_'+entry.name\n",
    "#                print('{} -> {}'.format(image_path, save_path))\n",
    "                transpose_image(image_path, save_path, transpose_1st=transpose_1st, transpose_2nd=transpose_2nd, rotate_1st=rotate_1st, rotate_2nd=rotate_2nd)\n",
    "        else:\n",
    "            #first create images with 1st transform/rotate and then second set of images with 2nd transform\n",
    "            #iterators in Python run only once, so we save the input and creaet new iters for each loop\n",
    "            files = list(class_it)\n",
    "            for entry in iter(files):\n",
    "                image_path = output_images_root+'/'+class_id+'/'+entry.name\n",
    "                save_path = output_images_root+'/'+class_id+'/'+prefix+'_1_'+entry.name\n",
    "#                print('{} -> {}'.format(image_path, save_path))\n",
    "                transpose_image(image_path, save_path, transpose_1st=transpose_1st, rotate_1st=rotate_1st)\n",
    "            for entry in iter(files):\n",
    "                image_path = output_images_root+'/'+class_id+'/'+entry.name\n",
    "                save_path = output_images_root+'/'+class_id+'/'+prefix+'_2_'+entry.name\n",
    "#                print('{} -> {}'.format(image_path, save_path))\n",
    "                transpose_image(image_path, save_path, transpose_2nd=transpose_2nd, rotate_2nd=rotate_2nd)\n",
    "\n",
    "\n",
    "def duplicate_class_horizontally(class_id_1, class_id_2):\n",
    "    class_it_1 = list(os.scandir(output_images_root+'/'+class_id_1))\n",
    "    class_it_2 = list(os.scandir(output_images_root+'/'+class_id_2))\n",
    "    for entry_1 in iter(class_it_1):\n",
    "        os.system('cp {} {}'.format(output_images_root+'/'+class_id_1+'/'+entry_1.name,\n",
    "                                    output_images_root+'/'+class_id_2+'/'+'from_'+class_id_1+'_'+entry_1.name))\n",
    "    for entry_2 in iter(class_it_2):\n",
    "        os.system('cp {} {}'.format(output_images_root+'/'+class_id_2+'/'+entry_2.name,\n",
    "                                    output_images_root+'/'+class_id_1+'/'+'from_'+class_id_2+'_'+entry_2.name))\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_images_root = './source_data_2/train'\n",
    "h_sym = ['11', '12', '13', '15', '17', '18', '22', '26', '30', '35']\n",
    "v_sym = ['01', '05', '12', '15', '17']\n",
    "r_120_240 = ['40', '15']\n",
    "hv_flip = ['32']\n",
    "h_duplicate = [['19','20'], ['33','34'], ['36','37'], ['38','39']]\n",
    "\n",
    "#Test\n",
    "#output_images_root = './source_data_3/train'\n",
    "#h_sym = ['1']\n",
    "#v_sym = ['2']\n",
    "#r_120_240 = ['3', '4']\n",
    "#hv_flip = ['5']\n",
    "#h_duplicate = [['1','2'], ['3','4']]\n",
    "\n",
    "progress_bar1 = tqdm(range(len(h_sym)), desc='Flipping horizontally')\n",
    "for class_id in h_sym:\n",
    "    transpose_class(class_id, prefix='hflip', transpose_1st=Image.FLIP_LEFT_RIGHT, together=True)\n",
    "    progress_bar1.update()\n",
    "\n",
    "progress_bar2 = tqdm(range(len(v_sym)), desc='Flipping vertically')\n",
    "for class_id in v_sym:\n",
    "    transpose_class(class_id, prefix='vflip', transpose_1st=Image.FLIP_TOP_BOTTOM, together=True)\n",
    "    progress_bar2.update()\n",
    "\n",
    "progress_bar3 = tqdm(range(len(r_120_240)), desc='Rotating')\n",
    "for class_id in r_120_240:\n",
    "    transpose_class(class_id, prefix='rot', rotate_1st=120, rotate_2nd=240, together=False)\n",
    "    progress_bar3.update()\n",
    "\n",
    "progress_bar4 = tqdm(range(len(hv_flip)), desc='Flipping horiz and vert')\n",
    "for class_id in hv_flip:\n",
    "    transpose_class(class_id, prefix='hv_flip', \n",
    "                    transpose_1st=Image.FLIP_LEFT_RIGHT, \n",
    "                    transpose_2nd=Image.FLIP_TOP_BOTTOM, together=True)\n",
    "    progress_bar4.update()\n",
    "    \n",
    "progress_bar5 = tqdm(range(len(h_duplicate)), desc='Copying to symmetric class')\n",
    "for pair in h_duplicate:\n",
    "    duplicate_class_horizontally(pair[0], pair[1])  \n",
    "    progress_bar5.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how class count looks after these changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check data distribution\n",
    "\n",
    "train_dataset_2 = datasets.ImageFolder('./source_data_2/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros(len(train_dataset_2.classes))\n",
    "progress_bar = tqdm(range(len(train_dataset_2)), desc='Counting files in each class')\n",
    "\n",
    "for i in range(len(train_dataset_2)):\n",
    "    x,y = train_dataset_2[i]\n",
    "    labels[y] +=1\n",
    "    progress_bar.update()\n",
    "labels = np.array(labels, dtype=int)\n",
    "\n",
    "print('Smallest class: '+str(min(labels)))\n",
    "print('Biggest class:  '+str(max(labels)))\n",
    "\n",
    "classes = np.arange(len(labels))\n",
    "plt.bar(classes, labels)\n",
    "plt.xlabel('Signs')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of signs in each class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have much more training data but some small classes didn't change, for example \"20 speed limit\". \n",
    "We should try to further augment the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_count = get_track_count('./source_data_2/train/')\n",
    "print('Lowest number of tracks per class: '+str(min(track_count['track_count'])))\n",
    "print('Highest number of tracks per class:  '+str(max(track_count['track_count'])))\n",
    "\n",
    "classes = np.arange(len(track_count['track_count']))\n",
    "plt.bar(classes, track_count['track_count'], color='green')\n",
    "plt.xlabel('Tracks')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of tracks in each class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split images into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_root = './source_data_2/train/'\n",
    "valid_images_root = './source_data_2/valid/'\n",
    "\n",
    "def train_valid_split(train_images_root, valid_images_root, train_validation_ratio = 0.15):\n",
    "    os.makedirs(valid_images_root, exist_ok=True)\n",
    "    progress_bar_classes = tqdm(range(len(track_count)), desc='Extracting validation tracks')\n",
    "    for index, row in track_count.iterrows():\n",
    "        class_id = row['class_id']\n",
    "        #select at least 2 tracks for validataion set\n",
    "        valid_count= max(2,round(row['track_count'] * train_validation_ratio))\n",
    "        os.makedirs(valid_images_root+class_id, exist_ok=True)\n",
    "        with os.scandir(train_images_root + class_id) as images_it:\n",
    "            images = list(images_it)\n",
    "            images_tracks = pd.DataFrame(images)\n",
    "            images_tracks['track'] = [image.name[:image.name.rfind(\"_\")] for image in images_tracks[0]]\n",
    "            sampled_track_list = random.sample(list(images_tracks['track']), valid_count)\n",
    "            for sampled_track in iter(sampled_track_list):\n",
    "                sampled_images=images_tracks[images_tracks['track']==sampled_track][0]\n",
    "                for image in sampled_images:\n",
    "                    os.system('mv {} {}'.format(train_images_root+class_id+'/'+image.name,\n",
    "                                            valid_images_root+class_id+'/'+image.name))\n",
    "        progress_bar_classes.update()\n",
    "                    \n",
    "train_valid_split(train_images_root, valid_images_root, train_validation_ratio = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For test set we will use GTSRB Test set that we downloaded to source_data/test.\n",
    "This dataset comprises of files all in one directory and a CSV file with Ground Truth labels.\n",
    "We will copy it to the same file structure as /train and /valid sets, also using the same class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_input_path = './source_data/test/GTSRB/Final_Test/Images/'\n",
    "test_lables_input_path = './source_data/test/GT-final_test.csv'\n",
    "test_images_output_path  = './source_data_2/test/'\n",
    "\n",
    "def prepare_test_data(test_images_input_path, test_lables_input_path, test_images_output_path):\n",
    "    gt_data = pd.read_csv(test_lables_input_path, header=0, sep=';')\n",
    "    gt_data['_00ClassId'] = gt_data.ClassId.astype(str).str.zfill(2)\n",
    "    classes = pd.unique(gt_data['_00ClassId'])\n",
    "    for class_id in pd.unique(gt_data['_00ClassId']):\n",
    "        os.makedirs(test_images_output_path+class_id, exist_ok=True)\n",
    "\n",
    "    progress_bar = tqdm(range(len(gt_data.index)), desc='Copying test classes')\n",
    "    for index, row in gt_data.iterrows():\n",
    "        class_id = row['_00ClassId']\n",
    "        os.system('cp {} {}'.format(test_images_input_path+row['Filename'],\n",
    "                                    test_images_output_path+class_id+'/'+row['Filename']))\n",
    "        progress_bar.update()\n",
    "\n",
    "        \n",
    "prepare_test_data(test_images_input_path, test_lables_input_path, test_images_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDT = datetime.datetime.now()\n",
    "print (str(currentDT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
