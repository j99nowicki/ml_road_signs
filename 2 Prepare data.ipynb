{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road signs classifier - preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total run time: approx. 20 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download and inspect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using GTSRB dataset (German Traffic Sign Recognition Benchmark) from http://benchmark.ini.rub.de/?section=gtsrb&subsection=news.\n",
    "Available datasets are train and test. We will further split train set to train and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-01 11:51:50.448588\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "currentDT = datetime.datetime.now()\n",
    "print (str(currentDT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods\n",
    "\n",
    "def get_sign_count(dataset):\n",
    "    labels = np.zeros(len(dataset.classes))\n",
    "    progress_bar = tqdm(range(len(dataset)), desc='Counting files in each class')\n",
    "    for i in range(len(dataset)):\n",
    "        x,y = dataset[i]\n",
    "        labels[y] +=1\n",
    "        progress_bar.update()\n",
    "    count = np.array(labels, dtype=int)\n",
    "    return count\n",
    "\n",
    "def vis_sign_count(count):\n",
    "    print('Smallest class: '+str(min(count)))\n",
    "    print('Biggest class:  '+str(max(count)))\n",
    "\n",
    "    classes = np.arange(len(count))\n",
    "    plt.bar(classes, count)\n",
    "    plt.xlabel('Signs')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Count of signs in each class')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tack helper methods\n",
    "\n",
    "def get_track_count(input_images_root):\n",
    "    track_count = pd.DataFrame(columns=['class_id','track_count'])\n",
    "    with os.scandir(input_images_root) as images_it:\n",
    "        progress_bar = tqdm(range(43), desc='Counting tracks in each class')\n",
    "        for class_id in images_it:\n",
    "            if class_id.is_dir():# and class_id.name.startswith('00000'):\n",
    "                with os.scandir(input_images_root+'/'+class_id.name) as class_it:\n",
    "                    #print(input_images_root+'/'+class_id.name)\n",
    "                    df = pd.DataFrame(columns=['name'])\n",
    "                    for entry in class_it:\n",
    "                        if entry.is_file() and entry.name.endswith('.ppm'):\n",
    "                            df = df.append({'name':entry.name, 'track':entry.name[:entry.name.rfind(\"_\")]}, ignore_index=True) \n",
    "                    track_count = track_count.append({'class_id':class_id.name, 'track_count':len(df.track.unique())}, ignore_index=True)        \n",
    "            progress_bar.update()\n",
    "    track_count = track_count.sort_values(by=['class_id'])\n",
    "    return track_count\n",
    "\n",
    "def vis_track_count(track_count):\n",
    "    print('Lowest number of tracks per class: '+str(min(track_count['track_count'])))\n",
    "    print('Highest number of tracks per class:  '+str(max(track_count['track_count'])))\n",
    "    \n",
    "    classes = np.arange(len(track_count['track_count']))\n",
    "    plt.bar(classes, track_count['track_count'], color='green')\n",
    "    plt.xlabel('Tracks')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Count of tracks in each class')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation by reuse and rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the overall size of the data set, when taking into account a high number of classes and the correlation within tracks, the data is smaller than it seems. As noticed by some researchers (for example [here](https://medium.com/@wolfapple/traffic-sign-recognition-2b0c3835e104), although the author made an error for the roundabout sign) it can be extended by rotating some signs or flipping and reusing for other class. For example left and right turn signs can be reused, and give way sign has horizontal symetry.<br>\n",
    "<br>\n",
    "Classes which can reuse images flipped horizontally from other class: (19,20), (33,34), (36,37), (38,39)<br>\n",
    "Classes with horizontal symmetry: 11, 12, 13, 15, 17, 18, 22, 26, 30, 35<br>\n",
    "Classes with vertical symmetry: 01, 05, 12, 15, 17<br>\n",
    "Classes which can be rotated by 120 or 240 degrees: 40<br>\n",
    "Classes which can be rotated by any degree: 15<br>\n",
    "Classes which diagonal symetry : 32<br>\n",
    "<br>\n",
    "Let's create a new data source directory which will include reused images.<br> \n",
    "I'll also change classes to 2 digits in the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14294181cd2745e89cd5262b2f32911b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Copying files in each class', max=43.0, style=ProgressStyâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first copy all .ppm files to new location using new class names\n",
    "input_images_root = './source_data/train/GTSRB/Final_Training/Images'\n",
    "output_images_root = './source_data_2/train'\n",
    "\n",
    "os.makedirs(output_images_root, exist_ok=True)\n",
    "with os.scandir(input_images_root) as images_it:\n",
    "    progress_bar = tqdm(range(43), desc='Copying files in each class')\n",
    "    for class_id in images_it:\n",
    "        if class_id.is_dir(): #and class_id.name.startswith('00000'):\n",
    "            os.makedirs(output_images_root+'/'+class_id.name[-2:], exist_ok=True)\n",
    "            with os.scandir(input_images_root+'/'+class_id.name) as class_it:\n",
    "                #print(input_images_root+'/'+class_id.name)\n",
    "                for entry in class_it:\n",
    "                    if entry.is_file() and entry.name.endswith('.ppm'):\n",
    "                        os.system('cp {} {}'.format(input_images_root+'/'+class_id.name+'/'+entry.name,\n",
    "                                 output_images_root+'/'+class_id.name[-2:]+'/'+entry.name))  \n",
    "            progress_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transforms\n",
    "\n",
    "def transpose_image(image_path, save_path, transpose_1st=None, transpose_2nd=None, rotate_1st=None, rotate_2nd=None):\n",
    "    \"\"\"\n",
    "    Transpose and rotate the given photo once or twice and save it\n",
    "    @param image_path: The path to the input image\n",
    "    @param transpose_1st: one of \n",
    "    @param transpose_2nd: one of \n",
    "        PIL.Image.FLIP_LEFT_RIGHT, \n",
    "        PIL.Image.FLIP_TOP_BOTTOM, \n",
    "        PIL.Image.ROTATE_90, \n",
    "        PIL.Image.ROTATE_180, \n",
    "        PIL.Image.ROTATE_270 or \n",
    "        PIL.Image.TRANSPOSE\n",
    "    @param rotate_1st: degree to rotate, anti-clockwise \n",
    "    @param rotate_2nd: degree to rotate, anti-clockwise \n",
    "    @param save_path: Path to save the image\n",
    "    \"\"\"\n",
    "    image_in = Image.open(image_path)\n",
    "    if transpose_1st != None:\n",
    "        image_out = image_in.transpose(transpose_1st)\n",
    "        image_in = image_out \n",
    "    if transpose_2nd != None:\n",
    "        image_out = image_in.transpose(transpose_2nd)\n",
    "        image_in = image_out \n",
    "    if rotate_1st != None:\n",
    "        image_out = image_in.rotate(rotate_1st)\n",
    "        image_in = image_out \n",
    "    if rotate_2nd != None:\n",
    "        image_out = image_in.rotate(rotate_2nd)\n",
    "    image_out.save(save_path)\n",
    "\n",
    "\n",
    "\n",
    "def transpose_class(class_id, prefix, transpose_1st=None, transpose_2nd=None, rotate_1st=None, rotate_2nd=None, together=True):\n",
    "    with os.scandir(output_images_root+'/'+class_id) as class_it:\n",
    "        if together:\n",
    "            #apply both transforms or rotations to each file\n",
    "            for entry in class_it:\n",
    "                image_path = output_images_root+'/'+class_id+'/'+entry.name\n",
    "                save_path = output_images_root+'/'+class_id+'/'+prefix+'_'+entry.name\n",
    "#                print('{} -> {}'.format(image_path, save_path))\n",
    "                transpose_image(image_path, save_path, transpose_1st=transpose_1st, transpose_2nd=transpose_2nd, rotate_1st=rotate_1st, rotate_2nd=rotate_2nd)\n",
    "        else:\n",
    "            #first create images with 1st transform/rotate and then second set of images with 2nd transform\n",
    "            #iterators in Python run only once, so we save the input and creaet new iters for each loop\n",
    "            files = list(class_it)\n",
    "            for entry in iter(files):\n",
    "                image_path = output_images_root+'/'+class_id+'/'+entry.name\n",
    "                save_path = output_images_root+'/'+class_id+'/'+prefix+'_1_'+entry.name\n",
    "#                print('{} -> {}'.format(image_path, save_path))\n",
    "                transpose_image(image_path, save_path, transpose_1st=transpose_1st, rotate_1st=rotate_1st)\n",
    "            for entry in iter(files):\n",
    "                image_path = output_images_root+'/'+class_id+'/'+entry.name\n",
    "                save_path = output_images_root+'/'+class_id+'/'+prefix+'_2_'+entry.name\n",
    "#                print('{} -> {}'.format(image_path, save_path))\n",
    "                transpose_image(image_path, save_path, transpose_2nd=transpose_2nd, rotate_2nd=rotate_2nd)\n",
    "\n",
    "\n",
    "def duplicate_class_horizontally(class_id_1, class_id_2):\n",
    "    class_it_1 = list(os.scandir(output_images_root+'/'+class_id_1))\n",
    "    class_it_2 = list(os.scandir(output_images_root+'/'+class_id_2))\n",
    "    for entry_1 in iter(class_it_1):\n",
    "        os.system('cp {} {}'.format(output_images_root+'/'+class_id_1+'/'+entry_1.name,\n",
    "                                    output_images_root+'/'+class_id_2+'/'+'from_'+class_id_1+'_'+entry_1.name))\n",
    "    for entry_2 in iter(class_it_2):\n",
    "        os.system('cp {} {}'.format(output_images_root+'/'+class_id_2+'/'+entry_2.name,\n",
    "                                    output_images_root+'/'+class_id_1+'/'+'from_'+class_id_2+'_'+entry_2.name))\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd798840a8a4e8f94b39e29e7843f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Flipping horizontally', max=10.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad40653485646d49e6d08ff7021e82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Flipping vertically', max=5.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3218b0eb47402d91359552413cb3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Rotating', max=2.0, style=ProgressStyle(description_widthâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4343801e3b4a7cb08a53ea76085168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Flipping horiz and vert', max=1.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35dbded22079403c93f6720d27bf782e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Copying to symmetric class', max=4.0, style=ProgressStyleâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "output_images_root = './source_data_2/train'\n",
    "h_sym = ['11', '12', '13', '15', '17', '18', '22', '26', '30', '35']\n",
    "v_sym = ['01', '05', '12', '15', '17']\n",
    "r_120_240 = ['40', '15']\n",
    "hv_flip = ['32']\n",
    "h_duplicate = [['19','20'], ['33','34'], ['36','37'], ['38','39']]\n",
    "\n",
    "#Test\n",
    "#output_images_root = './testing_transforms/train'\n",
    "#h_sym = ['1']\n",
    "#v_sym = ['2']\n",
    "#r_120_240 = ['3', '4']\n",
    "#hv_flip = ['5']\n",
    "#h_duplicate = [['1','2'], ['3','4']]\n",
    "\n",
    "progress_bar1 = tqdm(range(len(h_sym)), desc='Flipping horizontally')\n",
    "for class_id in h_sym:\n",
    "    transpose_class(class_id, prefix='hflip', transpose_1st=Image.FLIP_LEFT_RIGHT, together=True)\n",
    "    sleep(0.01) # progress bar sometimes don't update if update() is called too quickly\n",
    "    progress_bar1.update()\n",
    "\n",
    "progress_bar2 = tqdm(range(len(v_sym)), desc='Flipping vertically')\n",
    "for class_id in v_sym:\n",
    "    transpose_class(class_id, prefix='vflip', transpose_1st=Image.FLIP_TOP_BOTTOM, together=True)\n",
    "    sleep(0.01)\n",
    "    progress_bar2.update()\n",
    "\n",
    "progress_bar3 = tqdm(range(len(r_120_240)), desc='Rotating')\n",
    "for class_id in r_120_240:\n",
    "    transpose_class(class_id, prefix='rot', rotate_1st=120, rotate_2nd=240, together=False)\n",
    "    sleep(0.01)\n",
    "    progress_bar3.update()\n",
    "\n",
    "progress_bar4 = tqdm(range(len(hv_flip)), desc='Flipping horiz and vert')\n",
    "for class_id in hv_flip:\n",
    "    transpose_class(class_id, prefix='hv_flip', \n",
    "                    transpose_1st=Image.FLIP_LEFT_RIGHT, \n",
    "                    transpose_2nd=Image.FLIP_TOP_BOTTOM, together=True)\n",
    "    sleep(0.1)\n",
    "    progress_bar4.update()\n",
    "    \n",
    "progress_bar5 = tqdm(range(len(h_duplicate)), desc='Copying to symmetric class')\n",
    "for pair in h_duplicate:\n",
    "    duplicate_class_horizontally(pair[0], pair[1])\n",
    "    sleep(0.01)\n",
    "    progress_bar5.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how class count looks after these changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check data distribution after data reuse\n",
    "\n",
    "train_dataset_2 = datasets.ImageFolder('./source_data_2/train/')\n",
    "sign_count_2 = get_sign_count(train_dataset_2)\n",
    "vis_sign_count(sign_count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_count = get_track_count('./source_data_2/train/')\n",
    "vis_track_count(track_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have much more training data but some small classes didn't change, for example \"20 speed limit\". We will need to take class imbalance into consideration during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train set into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_root = './source_data_2/train/'\n",
    "valid_images_root = './source_data_2/valid/'\n",
    "\n",
    "def train_valid_split(train_images_root, valid_images_root, train_validation_ratio = 0.1, min_track = 1):\n",
    "    os.makedirs(valid_images_root, exist_ok=True)\n",
    "    progress_bar_classes = tqdm(range(len(track_count)), desc='Extracting validation tracks')\n",
    "    for index, row in track_count.iterrows():\n",
    "        class_id = row['class_id']\n",
    "        #select at least min_track tracks for validataion set\n",
    "        valid_count= max(min_track,round(row['track_count'] * train_validation_ratio))\n",
    "        os.makedirs(valid_images_root+class_id, exist_ok=True)\n",
    "        with os.scandir(train_images_root + class_id) as images_it:\n",
    "            images = list(images_it)\n",
    "            images_tracks = pd.DataFrame(images)\n",
    "            images_tracks['track'] = [image.name[:image.name.rfind(\"_\")] for image in images_tracks[0]]\n",
    "            sampled_track_list = random.sample(list(images_tracks['track']), valid_count)\n",
    "            for sampled_track in iter(sampled_track_list):\n",
    "                sampled_images=images_tracks[images_tracks['track']==sampled_track][0]\n",
    "                for image in sampled_images:\n",
    "                    os.system('mv {} {}'.format(train_images_root+class_id+'/'+image.name,\n",
    "                                            valid_images_root+class_id+'/'+image.name))\n",
    "        progress_bar_classes.update()\n",
    "                    \n",
    "train_valid_split(train_images_root, valid_images_root, train_validation_ratio = 0.1, min_track = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save final train weights, after validation tracks are extracted, for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_2_final = datasets.ImageFolder('./source_data_2/train/')\n",
    "sign_count_2_final = get_sign_count(train_dataset_2_final)\n",
    "\n",
    "#save weights for later use\n",
    "#weights = 1 / np.array([sign_count_2_final[y] for y in range(43)])\n",
    "#weights_df = pd.DataFrame(weights, columns=['weight'])\n",
    "#weights_df.to_csv('weights.csv', index_label='class_id')\n",
    "\n",
    "#save sign count for later use\n",
    "sign_count_2_final_df = pd.DataFrame(sign_count_2_final, columns=['count'])\n",
    "sign_count_2_final_df.to_csv('sign_count.csv', index_label='class_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For test set we will use GTSRB Test set that we downloaded to source_data/test.\n",
    "This dataset comprises of files all in one directory and a CSV file with Ground Truth labels.\n",
    "We will copy it to the same file structure as /train and /valid sets, also using the same class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_input_path = './source_data/test/GTSRB/Final_Test/Images/'\n",
    "test_lables_input_path = './source_data/test/GT-final_test.csv'\n",
    "test_images_output_path  = './source_data_2/test/'\n",
    "\n",
    "def prepare_test_data(test_images_input_path, test_lables_input_path, test_images_output_path):\n",
    "    gt_data = pd.read_csv(test_lables_input_path, header=0, sep=';')\n",
    "    gt_data['_00ClassId'] = gt_data.ClassId.astype(str).str.zfill(2)\n",
    "    classes = pd.unique(gt_data['_00ClassId'])\n",
    "    for class_id in pd.unique(gt_data['_00ClassId']):\n",
    "        os.makedirs(test_images_output_path+class_id, exist_ok=True)\n",
    "\n",
    "    progress_bar = tqdm(range(len(gt_data.index)), desc='Copying test classes')\n",
    "    for index, row in gt_data.iterrows():\n",
    "        class_id = row['_00ClassId']\n",
    "        os.system('cp {} {}'.format(test_images_input_path+row['Filename'],\n",
    "                                    test_images_output_path+class_id+'/'+row['Filename']))\n",
    "        progress_bar.update()\n",
    "\n",
    "        \n",
    "prepare_test_data(test_images_input_path, test_lables_input_path, test_images_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has high imbalance so we risk that the model would favour bigger classes as more probable based on input distribution, not features. The imbalace got even worse after reusing images because some small classes like \"speed limit to 20\" could not benefit from it.<br>\n",
    "Below we can see count of signs in each class before and after reusing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sign_count = get_sign_count(train_dataset)\n",
    "vis_sign_count(sign_count)\n",
    "vis_sign_count(sign_count_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to address it. One would be to add weights to loss function during training (for example https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss). However with over 10x difference between biggest and smallest classes, rare samples from small classes would be giving high error input to back propagation and the model might be difficult to train. <br>\n",
    "Another approach could be data augmentation based on the loss. Traffic signs are good candidates for augmentation because we can apply some limited affine transformations or rotate by small angles. <br>\n",
    "Ideally, the augmentation should be applied on training set to equalize class sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDT = datetime.datetime.now()\n",
    "print (str(currentDT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
